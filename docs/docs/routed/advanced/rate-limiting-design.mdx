---
id: rate-limiting-design
title: Rate limiting design notes
sidebar_position: 6
description: Planned architecture for Routed’s token-bucket limiter, backends, and integration points.
---

This design note captures the scope and architecture for Routed’s upcoming rate limiting subsystem. Treat it as the north star while the implementation lands across multiple milestones.

## Goals

- Protect applications from abusive clients without resorting to reverse proxies.
- Cover the common throttling shapes: per-route, per-group, per-identity (IP, user ID, API key).
- Offer at least two backends (in-memory + Redis) with identical configuration semantics.
- Provide actionable responses (`429` with `Retry-After`) and telemetry so operators can observe enforcement.
- Integrate cleanly with other middleware (auth, compression, logging) without surprising ordering constraints.

## Architecture snapshot

| Component | Responsibility | Notes |
| --------- | -------------- | ----- |
| `RateLimitMiddleware` | Enforce policies for the current request. | Reads a compiled policy set from the provider and decides whether to allow, delay, or reject the request. |
| `RateLimitProvider` | Compiles config into policy definitions and wires up the chosen backend. | Lives alongside other providers (`routed.rate-limiting`), supports runtime reloads, and validates configuration shape. |
| Store adapters | Persist counters/tokens. | Memory adapter for single-node setups, Redis adapter for distributed deployments. The Redis adapter will use Lua scripts or atomic commands to avoid race conditions. |
| Policy compiler | Converts friendly config into deterministic policies. | Handles defaults, merging group overrides, and translating shorthand notation into token bucket parameters. |

Policies follow the token bucket model with optional leaky-bucket smoothing. Each policy contains:

- **Key** – derived from scope (route/group/global) and identity selectors (IP, header, session, custom lambda).
- **Capacity** – maximum number of tokens (requests) allowed in the window.
- **Refill** – rate at which tokens are added (per second/minute).
- **Burst** – optional burst multiplier to allow short spikes.

## Configuration sketch

```yaml title="config/rate-limiting.yaml"
enabled: true
default:
  backend: memory
  store: array
  capacity: 100
  refill: 60s
  burst: 2
keys:
  - name: ip
    type: ip
  - name: user
    type: header
    header: x-user-id
routes:
  - match: GET /api/v1/*
    limits:
      - key: ip
        capacity: 30
        refill: 60s
      - key: user
        capacity: 120
        refill: 60s
groups:
  - name: admin
    match: /admin/**
    burst: 1
    backend: redis
    store: redis
```

The provider merges `default`, `routes`, and `groups` into an ordered policy list. Per-route rules override group rules when both apply.

Stores come from the cache subsystem. Define them under `cache.stores` (for example, `driver: redis` with a connection URL) and reference the store name from `rate_limit.store`. This keeps limiter state aligned with the cache drivers already in use.

## Integration points

- **Middleware ordering:** The limiter should run after auth (so user identity is available) but before expensive handlers or compression. We will add an ordering helper similar to other middleware helpers.
- **Telemetry:** Emit `RateLimitAllowedEvent` and `RateLimitBlockedEvent` (with `failoverMode`) so observability systems can collect metrics. The Redis adapter will expose hit/miss counters, and the memory adapter will surface an in-process snapshot for dashboards.
- **CLI support:** Extend `routed provider:list` and new `provider:rate-limiter` commands to inspect policies, effective configuration, and backend health.
- **Testing utilities:** Ship helpers in `routed_testing` to simulate bursts and verify that limits trigger as expected without waiting for real time.

## Future enhancements

- **Distributed fairness beyond local failover:** The current `local` failover keeps enforcing limits per instance when Redis is down. Revisit consistent hashing or peer sharing if operational needs arise.
- **Advanced quota analytics:** Rolling quotas ship with a simple reset model. Explore cumulative reporting and proactive alerts once observability stories settle.
- **CLI ergonomics:** `provider:list --config` surfaces defaults today. We may still add a dedicated `provider:rate-limiter` inspector for live policy state if users request it.

## Next steps

1. Finalise configuration schema and provider validation rules.  
2. Implement memory backend and middleware integration with exhaustive tests.  
3. Add Redis backend with atomic scripts and failure recovery stories.  
4. Wire telemetry and CLI inspection commands.  
5. Documentation updates (this page becomes the canonical reference, plus quick-start guides in the Security section).

Contributions should link back to this design note so reviewers can trace decisions and confirm the implementation matches the agreed scope.
